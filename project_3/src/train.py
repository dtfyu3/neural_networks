import os
import shutil
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, models, transforms
from torch.utils.data import DataLoader, random_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import argparse

# ==========================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ==========================================
# –ú–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å —ç—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
DATA_DIR = '/content/data'     # –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º
MODEL_SAVE_PATH = 'plant_classifier_resnet18.pth'
PLOT_SAVE_PATH = 'training_plot.png'
CM_SAVE_PATH = 'confusion_matrix.png'
BATCH_SIZE = 32
EPOCHS = 10
LEARNING_RATE = 0.001

def get_device():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üöÄ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    if device.type == 'cpu':
        print("‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –í—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ CPU. –û–±—É—á–µ–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω—ã–º.")
    return device

def prepare_data(data_dir, batch_size):
    # 1. –û—á–∏—Å—Ç–∫–∞ –æ—Ç –º—É—Å–æ—Ä–∞ (ipynb_checkpoints)
    checkpoint_folder = os.path.join(data_dir, '.ipynb_checkpoints')
    if os.path.exists(checkpoint_folder):
        print(f"üßπ –£–¥–∞–ª–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–π –ø–∞–ø–∫–∏: {checkpoint_folder}")
        shutil.rmtree(checkpoint_folder)

    # 2. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ (–ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è)
    train_transforms = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(30),
        transforms.ColorJitter(brightness=0.2, contrast=0.2),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    val_transforms = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    print("‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    try:
        full_dataset = datasets.ImageFolder(data_dir, transform=train_transforms)
    except FileNotFoundError:
        print(f"‚ùå –û–®–ò–ë–ö–ê: –ü–∞–ø–∫–∞ {data_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        exit(1)

    class_names = full_dataset.classes
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Å–æ–≤: {len(class_names)}")
    
    # –†–∞–∑–±–∏–µ–Ω–∏–µ 80/20
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_data, val_data = random_split(full_dataset, [train_size, val_size])
    
    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º transform –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (—á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –Ω–∞ —Ç–µ—Å—Ç–µ)
    # –í ImageFolder transform –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫–æ –≤—Å–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É, –Ω–æ random_split
    # —Å–æ–∑–¥–∞–µ—Ç Subset. –≠—Ç–æ —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥, –¥–ª—è production –ª—É—á—à–µ —Å–æ–∑–¥–∞—Ç—å –¥–≤–∞ ImageFolder.
    val_data.dataset.transform = val_transforms 

    dataloaders = {
        'train': DataLoader(train_data, batch_size=batch_size, shuffle=True),
        'val': DataLoader(val_data, batch_size=batch_size, shuffle=False)
    }
    
    return dataloaders, class_names, len(train_data), len(val_data)

def build_model(num_classes, device):
    print("üõ† –ó–∞–≥—Ä—É–∑–∫–∞ ResNet18...")
    model = models.resnet18(pretrained=True)
    
    # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –≤–µ—Å–∞
    for param in model.parameters():
        param.requires_grad = False
    
    # –ú–µ–Ω—è–µ–º –≥–æ–ª–æ–≤—É
    model.fc = nn.Linear(model.fc.in_features, num_classes)
    model = model.to(device)
    return model

def train_model(model, dataloaders, criterion, optimizer, device, epochs):
    print(f"\nüèÉ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è ({epochs} —ç–ø–æ—Ö)...")
    history = {'train_acc': [], 'val_acc': []}
    
    for epoch in range(epochs):
        model.train()
        correct = 0
        total = 0
        
        for inputs, labels in dataloaders['train']:
            inputs, labels = inputs.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
            
        train_acc = correct / total
        history['train_acc'].append(train_acc)
        
        # Validation
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in dataloaders['val']:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
                correct += (preds == labels).sum().item()
                total += labels.size(0)
        
        val_acc = correct / total
        history['val_acc'].append(val_acc)
        
        print(f"Epoch {epoch+1}/{epochs} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}")
        
    return history

def evaluate_and_save_reports(model, dataloaders, device, class_names):
    print("\nüìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤...")
    model.eval()
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for inputs, labels in dataloaders['val']:
            inputs = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # 1. Text Report
    print("\n=== Classification Report ===")
    print(classification_report(all_labels, all_preds, target_names=class_names))
    
    # 2. Confusion Matrix Plot
    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.ylabel('True Class')
    plt.xlabel('Predicted Class')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig(CM_SAVE_PATH)
    print(f"üíæ –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ '{CM_SAVE_PATH}'")
    plt.close()

def main():
    device = get_device()
    dataloaders, class_names, train_len, val_len = prepare_data(DATA_DIR, BATCH_SIZE)
    
    model = build_model(len(class_names), device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.fc.parameters(), lr=LEARNING_RATE)
    
    # –õ–æ–≥–∏–∫–∞ Checkpointing (–ó–∞–≥—Ä—É–∂–∞—Ç—å –∏–ª–∏ –û–±—É—á–∞—Ç—å?)
    if os.path.exists(MODEL_SAVE_PATH):
        print(f"\nüíæ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏: {MODEL_SAVE_PATH}")
        print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤...")
        model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=device))
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        already_trained = True
    else:
        print("\n‚ö†Ô∏è –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–µ—Ç. –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è...")
        history = train_model(model, dataloaders, criterion, optimizer, device, EPOCHS)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ—Å–æ–≤
        torch.save(model.state_dict(), MODEL_SAVE_PATH)
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ '{MODEL_SAVE_PATH}'")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        plt.figure(figsize=(10, 6))
        plt.plot(history['train_acc'], label='Train Accuracy')
        plt.plot(history['val_acc'], label='Validation Accuracy')
        plt.title('Model Training History')
        plt.xlabel('Epochs')
        plt.ylabel('Accuracy')
        plt.legend()
        plt.grid(True)
        plt.savefig(PLOT_SAVE_PATH)
        print(f"üíæ –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ '{PLOT_SAVE_PATH}'")
        plt.close()
        already_trained = False

    # –í –ª—é–±–æ–º —Å–ª—É—á–∞–µ –ø—Ä–æ–≤–æ–¥–∏–º –æ—Ü–µ–Ω–∫—É
    evaluate_and_save_reports(model, dataloaders, device, class_names)

if __name__ == "__main__":
    main()